#!/usr/bin/env python3
"""

Prepare reference test data for Exodus-lambda integration test.
It requires a "data.yml" file specifying the reference test data.

"""

import argparse
import hashlib
import json
import os
import sys
import tempfile
from datetime import datetime

import boto3
import requests
import yaml
from tqdm import tqdm


class DBHandler:
    def __init__(self, table, config_table, session):
        self.dynamodb = session.client("dynamodb")
        self.table = table
        self.config_table = config_table

        self.default_from_date = datetime.utcnow().isoformat(
            timespec="seconds"
        )

    def put_item(
        self,
        web_uri,
        object_key,
        from_date=None,
        content_type=None,
        metadata={},
    ):
        if not from_date:
            from_date = self.default_from_date

        item = {
            "web_uri": {"S": web_uri},
            "from_date": {"S": from_date},
            "object_key": {"S": object_key},
            "metadata": {"M": metadata},
        }

        if content_type:
            item["content_type"] = {"S": content_type}

        self.dynamodb.put_item(
            TableName=self.table,
            Item=item,
        )

    def put_config(self, config, from_date=None):
        if not from_date:
            from_date = self.default_from_date

        item = {
            "from_date": {"S": from_date},
            "config_id": {"S": "exodus-config"},
            "config": {"S": json.dumps(config)},
        }

        self.dynamodb.put_item(
            TableName=self.config_table,
            Item=item,
        )


class S3Handler:
    def __init__(self, bucket, session):
        self.s3_client = session.client("s3")
        self.bucket = bucket

    def upload_from_localfile(self, path, checksum):
        st = os.stat(path)
        bar = tqdm(
            desc="%s [upload]" % checksum,
            total=st.st_size,
            unit="iB",
            unit_scale=True,
        )

        try:
            with open(path, "rb") as data:
                self.s3_client.upload_fileobj(
                    Fileobj=data,
                    Bucket=self.bucket,
                    Key=checksum,
                    Callback=bar.update,
                )
        finally:
            bar.close()


def parse_aws_session(parser):
    parser.add_argument(
        "--aws-access-id",
        default=None,
        help="Access ID for Amazon services. If no ID is provided, attempts to"
        " find it among environment variables and ~/.aws/config file will"
        " be made",
    )
    parser.add_argument(
        "--aws-access-key",
        default=None,
        help="Access key for Amazon services. If no key is provided, attempts"
        " to find it among environment variables and ~/.aws/config file"
        " will be made",
    )
    parser.add_argument(
        "--aws-session-token",
        default=None,
        help="Session token for Amazon services. If no token is provided,"
        " attempts to find it among environment variables and"
        " ~/.aws/config file will be made",
    )
    parser.add_argument(
        "--default-region",
        default="us-east-1",
        help="Default region for Amamzon services. If no region is provided,"
        " it will go with us-east-1",
    )


def parse_cdn_certs(parser):
    parser.add_argument(
        "--cert",
        default=os.path.expanduser("~/certs/rcm-debug/rcm-debug.crt"),
        type=str,
        help="Client certificate file and password",
    )
    parser.add_argument(
        "--key",
        default=os.path.expanduser("~/certs/rcm-debug/rcm-debug.key"),
        type=str,
        help="Private key file name",
    )
    parser.add_argument(
        "--cacert",
        default="/etc/rhsm/ca/redhat-uep.pem",
        type=str,
        help="CA certificate to verify peer against",
    )


def parse_args():
    root_parser = argparse.ArgumentParser(description=__doc__)
    subparsers = root_parser.add_subparsers(
        dest="command", help="reference test data operations"
    )
    # parser for prepare operation
    parser_prepare = subparsers.add_parser(
        "prepare",
        help="""
        prepare reference test data in dynamodb and s3
        for integration test,
         e.g. $./reftest prepare --bucket exodus-bucket --table exodus-table --config-table exodus-config
        """,
    )
    parser_prepare.add_argument(
        "--release-date",
        default=None,
        help="Date on which the content will be made available.",
    )
    parse_aws_session(parser_prepare)
    parse_cdn_certs(parser_prepare)
    parser_prepare.add_argument(
        "--bucket",
        required=True,
        help="The AWS S3 bucket used to store test data",
    )
    parser_prepare.add_argument(
        "--table",
        required=True,
        help="The AWS dynamoDB used to store test data",
    )
    parser_prepare.add_argument(
        "--config-table",
        required=True,
        help="The AWS dynamoDB used to store Exodus config",
    )

    return root_parser.parse_args()


class RefTestConfig:
    def __init__(self, prod_cdn_url, test_data, test_config):
        self.prod_cdn_url = prod_cdn_url
        self.test_data = test_data
        self.test_config = test_config


def load_config():
    with open("data.yml") as data_file:
        data = yaml.load(data_file, yaml.SafeLoader)

    with open("exodus-config.json") as config_file:
        config = json.load(config_file)

    return RefTestConfig(data["prod-cdn-url"], data["test_data"], config)


# It will return a TempFileObj and a checksum for test data verification
def download_to_local(url, key_path, cert_path, cacert_path):
    with requests.get(
        url,
        cert=(cert_path, key_path),
        verify=cacert_path,
        stream=True,
        timeout=(30, 30),
    ) as req:
        req.raise_for_status()
        total_size = int(req.headers.get("content-length", 0))
        tbar = tqdm(desc=url, total=total_size, unit="iB", unit_scale=True)

        # the file is deleted as soon as it is closed.
        temp_file = tempfile.NamedTemporaryFile(delete=True)

        sha256 = hashlib.sha256()

        for chunk in req.iter_content(chunk_size=8192):
            if chunk:  # filter out keep-alive new chunks
                tbar.update(len(chunk))
                temp_file.write(chunk)
                sha256.update(chunk)
                temp_file.flush()
        tbar.close()
        return temp_file, sha256.hexdigest()


def prepare(db_client, s3_client, config, opt):
    for item in config.test_data:
        if not item.get("deploy", True):
            # skip items that specify deploy=false
            continue

        url = config.prod_cdn_url + item["path"]
        # download test data to local with a name "NamedTemporaryFile"
        temp_file, cdn_data_checksum = download_to_local(
            url, opt.key, opt.cert, opt.cacert
        )

        # For unstable content which did not provide sha256, it will skip the
        # checksum verify
        if item.get("sha256") and cdn_data_checksum != item["sha256"]:
            print(
                "{} verify checksum failed, cdn_data_checksum is {}, ".format(
                    item["path"], cdn_data_checksum
                )
                + "but test_data_checksum is {}".format(item["sha256"])
            )
            return False

        # push test data to s3 and dynamodb
        s3_client.upload_from_localfile(temp_file.name, cdn_data_checksum)
        db_client.put_item(
            item["path"],
            cdn_data_checksum,
            from_date=opt.release_date,
            content_type=item.get("content-type", None),
        )

        # delete the NamedTemporaryFile
        temp_file.close()

    # deploy Exodus config to config table
    db_client.put_config(config.test_config)

    return True


def main():
    config = load_config()
    opt = parse_args()

    session = boto3.Session(
        aws_access_key_id=opt.aws_access_id,
        aws_secret_access_key=opt.aws_access_key,
        aws_session_token=opt.aws_session_token,
        region_name=opt.default_region,
    )

    db_client = DBHandler(table=opt.table, config_table=opt.config_table, session=session)
    s3_client = S3Handler(bucket=opt.bucket, session=session)

    res = False
    if opt.command == "prepare":
        res = prepare(db_client, s3_client, config, opt)

    if res:
        print("{} operation has finished successfully!".format(opt.command))
    else:
        print("Fatal error: {} operation is terminated".format(opt.command))
        return 1


if __name__ == "__main__":
    sys.exit(main())
